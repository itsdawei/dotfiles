# Gilmer18Game

[Motivating the Rules of the Game for Adversarial Example Research](https://arxiv.org/abs/1807.06732)

## Keywords/Definitions

Perturbation defense
: Defenses against unrealistically-restricted perturbations of correctly
handled input examples.

The standard rules
: The attacker receives a starting point that is a draw from the data
distribution; the adversary is allowed to apply a perturbation to this starting
point with $l_p$ norm up to some given $\epsilon$, with the goal of inducing
a specific or non-specific error.

Hardness inversion
: The reported robustness is higher for a strictly more powerful attacker,
i.e., higher robustness to white-box attacks than black-box attacks. Sign of
incomplete evaluation.

## Main Contribution

- Examine the relevance and realism of the particular subset of adversarial
  examples considered in the perturbation literature.
- Introduce a taxonomy of rules governing games between an attacker and
  defender in the context of a machine learning system.
- Addresses the importance of establishing a clear and consistent set of
  evaluation protocols for adversarial examples in machine learning.

Adversarial example arises from an abstract two-player game between an attacker
and a defender, where the defender tries to solve a prediction problem with
a model that has been learned from data.

Principles in making the game rules:

- must capture the essential properties of a realistic threat model.
- Non-trivial (no attack or defense should exist that is always successful and practical)
- The attacker should have meaningful restrictions on their capabilities.

**What knowledge does the attacker have about the machine learning system?**
White-box setting: full knowledge of the model internals and training data.
Black-box setting: system details are unknown, but the attacker may query the
system by asking it to label inputs. Further restriction may limit the number
of such queries.

**What is the action space of the attacker?**

- Indistinguishable perturbation: random input from distribution. Perturbation
  must not be detectable by a human.
- Content-preserving perturbation: random input from distribution. The
  attackers may make any perturbation to example they want, as long as the
  content is preserved.
  - Pay-per-view attack
  - Revenge porn attack
- Non-suspicious input: any input example that looks normal to a human.
  - Voice assistant attack
  - Face-ID attack
- Content-constrained input: any input example as long as it contains some content.
  - Email spam attack
  - Malware attack
- Unconstrained input: any input.
  - Voice assistant attack
  - Face-ID attack

**Who goes first and is the game repeated?**
Reactive vs. pro-active

## The Test Set Attack

The test set attack is simplistic and it makes no perturbation at all, rather
the adversary blithely hopes the random starting point from the data
distribution will be misclassified.
Methods that apply defenses that increase test error are more vulnerable to the
test set attack.

If we consider more realistic rule sets than the standard rules where attackers
have full control over the input to a classifier, then finding model errors
with only limited query access and no knowledge or expertise in machine
learning can still be quite easy (#NotHotDog).

## Conclusion

- Many adversarial perturbation research are not motivated by security models,
  and thus does not evaluate their work with any robustness metric.
- Future researchers should try to be more explicit about the threat model in
  their analysis. Alternatively, the literature should be reoriented to be
  machine learning contributions as opposed to security contributions.
