# Tutorial

https://adversarial-ml-tutorial.org/introduction/

## How to create an adversarial image to fool a classifier?
Normally, when we train a classifier, we want to minimize the average loss over
some training set $\{x_i \in \mathcal{X}, y_i \in \Z \}$, for $i = [m]$.
$$
\min_\theta \frac{1}{m} \sum_{i=1}^m l(h_\theta(x_i), y_i)
$$
where $h_theta$ is the hypothesis (model) parameterized by $\theta$.

> TODO: write this in terms of risk minimization

To form an adversarial example, instead of adjusting the image to minimize the
loss, we adjust the image to *maximize* the loss.
$$
\max_{\hat{x}} l(h_\theta(\hat{x}), y)
$$
where $\hat{x}$ denotes out adversarial example. However, we cannot optimize
$\hat{x}$ over arbitrary images. Instead, we should ensure that $\hat{x}$ is
close to our original input $x$.

One way to achieve this is by optimizing over perturbations of the original
input $x$.
$$
\max_{\delta \in \Delta} l(h_\theta(x + \delta), y)
$$
where $\Delta$ represents an allowable set of perturbations.

## Targeted Attack
Turns out we can not only fool a classifier into giving the wrong class but
also trick the classifier into classifying the image into any class that we
desire. The only difference is that instead of trying to simply maximize the
loss of the correct class, we maximize the loss of the correct class while also
minimizing the loss of the target class.
$$
\max_{\delta \in \Delta} l(h_\theta(x + \delta), y) - l(h_\theta(x + \delta), y_\mathrm{target})
$$

## Adversarial Risk
As an alternative to the traditional risk, the **adversarial risk** suffers the worst case lost
in some region around the sample point.
$$
R_\mathrm{adv}(h_\theta) = \mathbb{E}_{(x,y) \sim \mathcal{D}} \lbrack \max_{\delta \in \Delta(x) l(h_\theta(x + \delta), y)}\rbrack
$$
Similarly, we define the empirical adversarial risk.
$$
R_\mathrm{adv}(h_\theta, D) = \frac{1}{|D|}\sum{E}_{(x,y) \in D} \max_{\delta \in \Delta(x) l(h_\theta(x + \delta), y)}
$$

**What does this measure?** The average loss from the worse-case adversary (for this hypothesis).

**Why is this useful?** This is a more accurate estimate of the expected
performance of a classifier in an adversarial environment, where an adversary
with full knowledge of the classifier is capable of manipulating the input.
Another reason is that, even when we want to minimize the traditional risk, 
it is very difficult to draw samples i.i.d. from the true distribution.

Unlike humans, machine learning algorithms are not resilient to change in the environment.
> Idea: QD can improve resilience of an algorithm.

## Training adversarially robust classifiers
The task of training a classifier that is robust to adversarial attacks can be
achieved by minimizing the empirical adversarial risk.
$$
\min_\theta \frac{1}{|D_\mathrm{train}|} \sum_{(x,y) \in D_\mathrm{train}} \max_{\delta \in \Delta(x)} l(h_\theta(x + \delta), y)
$$

Using SGD, we would update $\theta$ according to its gradient.
$$
\theta \gets \theta - \frac{\alpha}{\beta} \sum_{(x,y) \in B} \nabla_\theta \max_{\delta \in \Delta} l(h_\theta(x + \delta), y)
$$

To compute the gradient of the inner term, we appeal to [Danskin's
theorem](https://en.wikipedia.org/wiki/Danskin%27s_theorem), which states that
the gradient of the inner function involving the maximization term is simply
given by the gradient of the function evaluate at the maximum.
In other words, let $\delta^*$ denote the optimum of the inner maximization
problem, then the gradient is given by
$$
\nabla_\theta \max_{\delta \in \Delta(x)} l(h_\theta(x + \delta), y) = \nabla_\theta l(h_\theta(x + \delta^*), y)
$$

There is a nice interplay between the challenge of finding an adversarial
example and the process of training a robust classifier. We iteratively compute
adversarial examples, and update the classifier based on these adversarial
examples (instead of the original data point).
1. For each $x,y \in B$, compute an adversarial example.
$$
\delta^*(s) = \argmax_{\delta\in\Delta(x)} l(h_\theta(x + \delta), y)
$$
1. Compute the gradient of the empirical adversarial risk, and update $\theta$.
$$
\theta \gets \theta - \frac{\alpha}{\beta} \sum_{(x,y) \in B} \nabla_\theta l(h_\theta(x + \delta^*), y)
$$

It turns out that this only works well when the inner maximization problem is
solved "well enough." If a poor approximate strategy is used to solve the inner
maximization, then there is a good chance that a slightly more exhaustive inner
maximization strategy will prove an effective attack.

**Every adversarial attack and defense are a method for approximately solving
the inner maximization and/or outer minimization problem respectively.**
